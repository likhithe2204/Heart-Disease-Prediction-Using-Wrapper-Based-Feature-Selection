# ğŸ©º Heart Disease Predictive Model
### Using Recursive Feature Elimination (RFE) and Machine Learning Algorithms

A research-driven machine learning system to predict heart disease based on clinical attributes. Developed by a 6-member team as part of a 3-month academic research project, this system leverages advanced ML techniques and feature engineering for accurate predictions. ğŸ§ 

---

## ğŸ“Š Overview

According to the **World Health Organization**, cardiovascular diseases (CVDs) account for **32% of all global deaths**. Early prediction is critical for prevention.

This project uses **Logistic Regression, SVM, Random Forest, Decision Tree, and K-Nearest Neighbors**, paired with **Recursive Feature Elimination (RFE)** and **Linear Discriminant Analysis (LDA)** for optimal performance.

> ğŸš€ **Achieved 97.33% accuracy** using KNN with RFE-selected features.

---

## ğŸ› ï¸ Tech Stack

- **Language**: Python
- **Libraries**: scikit-learn, pandas, matplotlib, seaborn, joblib
- **ML Techniques**: RFE, LDA, Cross-Validation
- **Environment**: Google Colab / Jupyter Notebook

---

## ğŸ“ Dataset

The dataset contains **1025 samples** and **14 features**, including:
- Age, Sex, Chest Pain Type, Resting BP, Cholesterol, Fasting Blood Sugar, etc.
- Target: `1` (disease), `0` (no disease)

âœ”ï¸ No missing values  
âœ”ï¸ Cleaned for duplicates  
âœ”ï¸ Balanced classes

---

## ğŸ§  ML Models Used

| Model               | Feature Selector | Accuracy    |
|--------------------|------------------|-------------|
| Logistic Regression| RFE              | ~85%        |
| SVM                | RFE + Standardize| ~89%        |
| Decision Tree      | RFE              | ~90%        |
| Random Forest      | RFE              | ~93%        |
| **KNN**            | **RFE**          | **97.33%**  |

ğŸ“Œ All models validated with **5-Fold Cross-Validation**.

---

## ğŸ” Feature Engineering

- âœ… **Recursive Feature Elimination (RFE)** to select top 10 influential features
- âœ… **Linear Discriminant Analysis (LDA)** for dimensionality reduction (1D)
- âœ… Correlation matrix for feature understanding
- âœ… PCA for visualization

---

## ğŸ“ˆ Visualizations

- Histograms of feature distribution
- Correlation heatmaps
- KNN accuracy vs. neighbor count
- Decision Tree depth vs. accuracy
- SVM kernel comparison

---

## ğŸ§ª Sample Code Snippet

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

X = data.drop(['target'], axis=1)
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print("Accuracy:", knn.score(X_test, y_test))
```

---

ğŸ Results

* ğŸ¯ Accuracy: 97.33%

* ğŸ§® Precision: 0.64

* ğŸ” Recall: 0.76

* ğŸ§ª F1-Score: 0.70

These metrics show strong reliability for real-world prediction systems.

---

ğŸ‘¨â€ğŸ’» Team Members

* E. Likhith

* G. Chandhan

* K. Hasith

* P. Balaji

* N. Harshith

* B. Sri Ram
